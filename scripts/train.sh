#!/bin/bash
DEVICES='0,1,2,3,4,5,6'
BATCHSIZE=12
EPOCH=100
LEARNING_RATE=0.0002
exp_name='bs'+$BATCHSIZE+'_lr'+$LEARNING_RATE+'_epoch'+$EPOCH

TORCH_DISTRIBUTED_DEBUG=DETAIL NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=$DEVICES torchrun --nproc_per_node=7 \
train.py --batch_size $BATCHSIZE --log $exp_name --epochs $EPOCH --lr $LEARNING_RATE